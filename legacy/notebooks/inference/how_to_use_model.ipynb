{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 08:43:22.504602: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-26 08:43:22.505265: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 300, 50)           468000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 300, 50)           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 300, 400)         401600    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 300, 8)           3208      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 872,808\n",
      "Trainable params: 872,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('../../models/tensorflow/NER_model_updated.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "This is an example of a full pipeline of a sentence that needs to be predicted.\n",
    "\n",
    "Steps:\n",
    "1. Apply pre-processing (replace the dots and the commas)\n",
    "2. Split the words\n",
    "3. Import the embeddings (created from the dataset)\n",
    "4. Pad the sequence\n",
    "5. Predict\n",
    "6. ???\n",
    "7. Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_dot(x):\n",
    "    pattern = '\\w\\.' # removes dots at the end of sentences... ex: \"we went there.\" -> \"we went there\"\n",
    "    finds = re.findall(pattern, x)\n",
    "    for find in finds:\n",
    "        x = re.sub(pattern, find[:-1], x)\n",
    "\n",
    "    pattern_2 = '\\.\\w' # removes dots at the end of sentences. ex: \"we went .there\" -> \"we went there\"\n",
    "    x = re.sub(pattern_2, '', x)\n",
    "\n",
    "    pattern_3 = '\\s\\.\\s' # removes dots at the end of sentences. ex: \"abc . abc\" -> \"abc abc\"\n",
    "    x = re.sub(pattern_3, '', x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n",
      "Original word:  0000 === Predicted Tag:  O\n",
      "Original word:  Bottles === Predicted Tag:  O\n",
      "Original word:  ( === Predicted Tag:  O\n",
      "Original word:  +/- === Predicted Tag:  Incoterms\n",
      "Original word:  05/05 === Predicted Tag:  O\n",
      "Original word:  %) === Predicted Tag:  O\n",
      "Original word:  of === Predicted Tag:  O\n",
      "Original word:  First === Predicted Tag:  O\n",
      "Original word:  Class === Predicted Tag:  O\n",
      "Original word:  Technical === Predicted Tag:  O\n",
      "Original word:  Machinery === Predicted Tag:  O\n",
      "Original word:  Sodapacked === Predicted Tag:  O\n",
      "Original word:  in === Predicted Tag:  O\n",
      "Original word:  blue === Predicted Tag:  O\n",
      "Original word:  crates === Predicted Tag:  O\n",
      "Original word:  labelled === Predicted Tag:  O\n",
      "Original word:  'Test === Predicted Tag:  O\n",
      "Original word:  Machine === Predicted Tag:  O\n",
      "Original word:  London' === Predicted Tag:  O\n",
      "Original word:  Unit === Predicted Tag:  O\n",
      "Original word:  Price === Predicted Tag:  O\n",
      "Original word:  Usd === Predicted Tag:  O\n",
      "Original word:  50 === Predicted Tag:  O\n",
      "Original word:  DELIVERY === Predicted Tag:  O\n",
      "Original word:  TERMS; === Predicted Tag:  O\n",
      "Original word:  CIF === Predicted Tag:  O\n",
      "Original word:  TORONTO === Predicted Tag:  O\n",
      "Original word:  HS === Predicted Tag:  O\n",
      "Original word:  Code === Predicted Tag:  O\n",
      "Original word:  55678334 === Predicted Tag:  O\n"
     ]
    }
   ],
   "source": [
    "# Sentence to predict\n",
    "# sentence = 'EXW QTY:    192 UNITS CONCENTRATE 7-UP, AS PER BENEFICIARY\\'S SALES QUOTATION NO. 790005454 DATED 02.01.2020'\n",
    "sentence = '00,000 Bottles ( +/- 05/05 %) of First Class Technical Machinery Soda, packed in blue crates labelled \\'Test Machine London\\' Unit Price: Usd 5,00 DELIVERY TERMS; CIF TORONTO HS    Code:    55678.334    '\n",
    "\n",
    "sentence = sentence.replace('_x005F_x000D_','')\n",
    "sentence = sentence.replace('\\n',' ')\n",
    "sentence = sentence.replace(':',' ')\n",
    "sentence = sentence.replace('+)','')\n",
    "sentence = re.sub('\\d-', '', sentence)\n",
    "sentence = re.sub('\\(\\d\\)', '', sentence)\n",
    "sentence = re.sub('\\\\d\\)', '', sentence)\n",
    "sentence = re.sub(',\\s', '', sentence)\n",
    "sentence = re.sub(',\\w', '', sentence)\n",
    "sentence = remove_dot(sentence)\n",
    "sentence = sentence.replace(' - ','')\n",
    "sentence = sentence.replace(',','')\n",
    "sentence = sentence.replace('++++++++++++++++++++++++++++++++++++','')\n",
    "sentence = sentence.strip()\n",
    "\n",
    "words = sentence.split()\n",
    "\n",
    "import pickle\n",
    "# Save the embeddings\n",
    "with open(\"../../models/embeddings/words_embeddings\", \"rb\") as fp: \n",
    "    word2idx = pickle.load(fp)\n",
    "\n",
    "\n",
    "# Embedding\n",
    "X = []\n",
    "for word in words:\n",
    "    if word in word2idx.keys():\n",
    "        X.append(word2idx[word])\n",
    "    else:\n",
    "        X.append(9359)\n",
    "X\n",
    "\n",
    "max_len = 300 # taken from ner_nn.ipynb\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(maxlen=max_len, sequences=[X], padding=\"post\", value=len(word2idx)-1)\n",
    "\n",
    "# # Apply the predict function\n",
    "output = model.predict(X)\n",
    "\n",
    "with open(\"../../models/embeddings/tags_embeddings\", \"rb\") as fp: \n",
    "    tag2idx = pickle.load(fp)\n",
    "tag2idx = list(tag2idx)\n",
    "tag2idx\n",
    "\n",
    "predicted_output = []\n",
    "for word in output[0]:\n",
    "    predicted_output.append(tag2idx[word.argmax()])\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print('Original word: ', words[i], '=== Predicted Tag: ', predicted_output[i])\n",
    "# predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9359, 9359, 2955,  586, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 6139, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        3630, 4046, 9359, 4270, 9359, 6000, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359, 9359,\n",
       "        9359, 9359, 9359]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Quantity': [],\n",
       " 'UnitPriceAmount': [],\n",
       " 'O': [],\n",
       " 'GoodsDescription': [],\n",
       " 'Incoterms': [],\n",
       " 'GoodsOrigin': [],\n",
       " 'Tolerance': [],\n",
       " 'HSCode': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionar = {}\n",
    "for key in tag2idx:\n",
    "    dictionar[key] = []\n",
    "dictionar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Quantity': [],\n",
       " 'UnitPriceAmount': [],\n",
       " 'O': ['0000',\n",
       "  'Bottles',\n",
       "  '(',\n",
       "  '05/05',\n",
       "  '%)',\n",
       "  'of',\n",
       "  'First',\n",
       "  'Class',\n",
       "  'Technical',\n",
       "  'Machinery',\n",
       "  'Sodapacked',\n",
       "  'in',\n",
       "  'blue',\n",
       "  'crates',\n",
       "  'labelled',\n",
       "  \"'Test\",\n",
       "  'Machine',\n",
       "  \"London'\",\n",
       "  'Unit',\n",
       "  'Price',\n",
       "  'Usd',\n",
       "  '50',\n",
       "  'DELIVERY',\n",
       "  'TERMS;',\n",
       "  'CIF',\n",
       "  'TORONTO',\n",
       "  'HS',\n",
       "  'Code',\n",
       "  '55678334'],\n",
       " 'GoodsDescription': [],\n",
       " 'Incoterms': ['+/-'],\n",
       " 'GoodsOrigin': [],\n",
       " 'Tolerance': [],\n",
       " 'HSCode': []}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = []\n",
    "for word in output[0]:\n",
    "    y_pred.append(tag2idx[word.argmax()])\n",
    "\n",
    "for word, prediction in zip(words, y_pred):\n",
    "    dictionar[prediction].append(word)\n",
    "dictionar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('conpend')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b02c2d9a69eb5992dd78842f785fa56e23ad9a15fb314195388d25b994c9f13f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
